---
title: Inferência Estatística e Verossimilhança
summary: O conceito de inferência estatística e o paradigma da verossimilhança
author: André Felipe Berdusco Menezes
date: '2019-07-27'
draft: false
categories:
  - inferência estatística
  - verossimilhança
output:
  blogdown::html_page:
    toc: true
    number_sections: false
    toc_depth: 1
---


## Introdução

Em sua essência a ciência estatística procura maneiras de mensurar a incerteza.
O objetivo principal da estatística é realizar inferências sobre uma população ou fenômeno a partir de uma amostra
aleatória. A inferência é conduzida considerando que o fenômeno pode ser representado por um modelo, isto é, uma simplificação de
uma realidade mais complexa.
Dizemos que um bom modelo deve compreender e retratar as principais características do fenômeno.

Em geral, o modelo estatístico contém uma componente estocástica que representa as incertezas do fenômeno
e quantidades fixas e desconhecidas denominadas parâmetros.
A inferência estatística é realizada sobre os parâmetros do modelo utilizando os dados amostrais.
Note que ao realizar inferências sobre os parâmetros, fazemos inferências sobre o modelo e, portanto, sobre o fenômeno
que aquele modelo supostamente descreve.

A inferência estatística pode ser realizada de três diferentes formas:

1. estimação pontual;
2. estimação intervalar;
3. testes de hipóteses.

Na literatura, várias abordagens para estimação pontual dos parâmetros foram propostas. Por exemplo, neste
[trabalho](http://www.scielo.br/scielo.php?script=sci_arttext&pid=S0101-74382018000300555&tlng=en)
em parceria com meu orientador, professor [Josmar Mazucheli](http://lattes.cnpq.br/8899185212821611) e o professor [Sanku Dey](https://www.researchgate.net/profile/Sanku_Dey),
discutimos onze métodos para estimação pontual dos parâmetros de um modelo estatístico adequado para fenômenos restritos ao intervalo $(0,1).$

## Verossimilhança

Desde que foi proposto por [Sir Ronald Fisher](https://en.wikipedia.org/wiki/Ronald_Fisher) em uma série de artigos durante o período de 1912 a 1934 o
método da máxima verossimilhança é seguramente o mais empregado em situações práticas.
Os estimadores de máxima verossimilhança (EMV) possuem propriedades desejáveis e são utilizados na construção
de intervalos de confiança e testes de hipóteses.

As propriedades que fazem dos estimadores de máxima verossimilhança atraentes são:

 - assintoticamente não viesados;

 - consistentes;

 - eficientes;

 - invariantes sob transformação;

 - possuem distribuição assintótica Normal.

Assumindo um modelo estatístico parametrizado por um vetor de parâmetros
$\mathbf{\theta} = (\theta_1, \ldots, \theta_p)^\top$ fixo e desconhecido, a função de verossimilhança,
$L(\mathbf{\theta} \mid \mathbf{x})$, é uma medida de plausibilidade com a amostra observada $\mathbf{x}$,
tendo como propósito transmitir informações dos dados sobre $\mathbf{\theta}$.
Vamos supor que $\mathbf{x} = (x_1, \ldots, x_n)^\top$ é uma amostra independente e identicamente
distribuída de uma variável aleatória $X$ cuja função densidade de probabilidade $f(x \mid \mathbf{\theta})$ é
indexada pelo vetor de parâmetros $\mathbf{\theta}$. Então, a função de verossimilhança é definida por
\begin{equation}\label{eq:likelihood}
L(\mathbf{\theta} \mid \mathbf{x}) = \prod_{i=1}^{n}\,f(x_i \mid \mathbf{\theta}).
\end{equation}

Dizemos que $\widehat{\mathbf{\theta}}$ é um estimador de máxima verossimilhança de $\mathbf{\theta}$
se $L(\widehat{\mathbf{\theta}} \mid \mathbf{x}) \geq L(\mathbf{\theta} \mid \mathbf{x}), \forall \ \mathbf{\theta}$.
Uma vez que a função logaritmo é monótona crescente, por questões de conveniência computacional lidamos com a função de log-verossimilhança, $\ell(\mathbf{\theta} \mid \mathbf{x}) = \log  L(\mathbf{\theta} \mid \mathbf{x})$, pois ambas levam ao mesmo ponto de máximo.
Usualmente, os EMVs não podem ser expressos em um fórmula analítica,
assim são obtidos maximizando numericamente a função log-verossimilhança.

A primeira derivada da função de log-verossimilhança com respeito a $\mathbf{\theta}_j, j  = 1, \ldots, p$, é denominada
de função escore de Fisher. É possível mostrar que
\begin{equation*}
\mathbb{E}\left[\dfrac{\textrm{d}}{\textrm{d}\mathbf{\theta}_j}\,\ell(\mathbf{\theta} \mid \mathbf{x})\right] = 0,
\end{equation*}
isto é, a função escore tem média zero. Note que a função escore é um vetor de primeiras derivadas parciais, uma para cada elemento de $\mathbf{\theta}$.
Se a função log-verossimilhança é côncava, pode-se encontrar o EMV $\widehat{\mathbf{\theta}}$ de
$\mathbf{\theta}$ igualando a função escore a zero e resolvendo o sistemas de equações.


Outra quantidade importante presente na teoria da verossimilhança é a matriz informação de Fisher esperada, definida por
\begin{equation}\label{eq:fisher-esp}
\mathbf{I}\left(\mathbf{\theta}\right) = \mathbb{E}\left [-\dfrac{\partial^2}{\partial\mathbf{\theta}_j\,\partial\mathbf{\theta}_k} \log f(x \mid \mathbf{\theta})\right]
\end{equation}
para $j, k = 1, \ldots, p$.

O inconveniente de se trabalhar com $\mathbf{I}$ é a dificuldade de obter uma expressão analítica para a esperança.
Sob certas condições de regularidades, a matriz informação de Fisher observada,
definida por
\begin{equation}\label{eq:fisher-obs}
\mathbf{H}\left(\mathbf{\theta}\right) = -\dfrac{\partial}{\partial\mathbf{\theta}_j\,\partial\mathbf{\theta}_k} \log f(x_i \mid \mathbf{\theta}), %\at[\big]{\mathbf{\theta}=\widehat{\mathbf{\theta}}}
\end{equation}
é um estimador consistente de $\mathbf{I}\left(\mathbf{\theta}\right)$.

O conceito de informação esta associado a curvatura da função de verossimilhança sendo que quanto maior a curvatura mais precisa é a informação contida na verossimilhança. Em outras palavras a matriz de informação de Fisher indica
a precisão das estimativas dos parâmetros, sendo portanto, imprescindível para construção de intervalos de confiança e testes de hipóteses.


## Considerações finais
Neste texto procurei esclarecer o conceito de inferência e o paradigma da verossimilhança.
Em um próximo momento irei explorar o conceito de viés e discutir alguns métodos
para correções de viés dos estimadores de máxima verossimilhança em amostras finitas.

## Referências
[An Introduction to Bartlett Correction and Bias Reduction](https://www.springer.com/gp/book/9783642552540)

[Statistical Inference](https://www.amazon.com.br/Statistical-Inference-George-Casella/dp/0534243126)
