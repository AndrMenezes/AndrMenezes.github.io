

<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>At its core, statistical science looks for ways to measure the uncertainty.
The main goal of statistics is to perform inference about a phenomena based on
a random sample from the population phenomena.
The inference is conducted considering that the phenomena can be represented by a model,
i.e., a simplification of a more complex reality. A good model must understand e describe the
main features of phenomena.</p>
<p>In general, a statistical model has a stochastic component that represents the uncertainties
of phenomena, and fixed and unknown quantities called parameters.
Inference statistics is performed on model parameters using sample data.
Note that when making inferences about parameters, we make inferences about the model and
therefore about the phenomenon that the model is supposed to describe.</p>
<p>Statistical inference can be performed in three different ways:</p>
<ol style="list-style-type: decimal">
<li><p>point estimation;</p></li>
<li><p>interval estimation;</p></li>
<li><p>hypothesis tests.</p></li>
</ol>
<p>Several approaches for point estimation are available in the literature. In this
joint <a href="http://www.scielo.br/scielo.php?script=sci_arttext&amp;pid=S0101-74382018000300555&amp;tlng=en">work</a>
with <a href="http://lattes.cnpq.br/8899185212821611">Josmar Mazucheli</a> and <a href="https://www.researchgate.net/profile/Sanku_Dey">Sanku Dey</a>,
we discussed eleven methods of point estimation for a statistical model suitable
for bounded phenomena.</p>
</div>
<div id="likelihood" class="section level2">
<h2>Likelihood</h2>
<p>Since it was proposed by the famous
<a href="https://en.wikipedia.org/wiki/Ronald_Fisher">Ronald Fisher</a> in a series of articles during
the period of 1912 to 1934 the maximum likelihood method is certainly the most used in
practical situations. The maximum likelihood estimators (MLE) have desirable properties and
are used in the construction of confidence interval and hypothesis testing.</p>
<p>The properties that make the MLE attractive are:</p>
<ul>
<li><p>asymptotically unbiased;</p></li>
<li><p>consistency;</p></li>
<li><p>efficiency;</p></li>
<li><p>invariance under transformation;</p></li>
<li><p>asymptotically normally distributed.</p></li>
</ul>
<p>Assuming that a statistical model is parameterized by the unknown and fixed parameter vector
<span class="math inline">\(\boldsymbol{\theta} = (\theta_1, \ldots, \theta_p)^\top\)</span>, the likelihood function
<span class="math inline">\(L(\boldsymbol{\theta} \mid \boldsymbol{x})\)</span>, is a measure of plausibility with the observed sample
<span class="math inline">\(\boldsymbol{x}\)</span>, with the purpose of transmitting data information about <span class="math inline">\(\boldsymbol{\theta}\)</span>.
The general definition is that the likelihood function is the joint probability function
seen as function of the parameter, i.e.,
<span class="math display">\[
L(\boldsymbol{\theta} \mid \boldsymbol{x}) = f(\boldsymbol{\theta} \mid \boldsymbol{x}).
\]</span></p>
<p>It should be important to emphasize that the function form of the likelihood function
depends of the assumptions of the probabilistic model. For instance, under independence of
observations the join distribution is given by the product of densities functions.
In contrast, under dependence of observations the joint distribution can be written by
conditional probabilities.</p>
<div id="the-i.i.d.-case" class="section level3">
<h3>The i.i.d. case</h3>
<p>Let be <span class="math inline">\(\boldsymbol{x} = (x_1, \ldots, x_n)^\top\)</span> an <strong>independent</strong> and <strong>identical distributed</strong>
random sample from the random variable <span class="math inline">\(X\)</span>, which the probability density function <span class="math inline">\(f(x \mid \boldsymbol{\theta})\)</span>
depends on the parameter vector <span class="math inline">\(\boldsymbol{\theta}\)</span>.
Then, the likelihood function is defined by
<span class="math display">\[\begin{equation}
L(\boldsymbol{\theta} \mid \boldsymbol{x}) = \prod_{i=1}^{n}\,f(x_i \mid \boldsymbol{\theta}).
\end{equation}\]</span></p>
<p>We say that <span class="math inline">\(\widehat{\boldsymbol{\theta}}\)</span> is the maximum likelihood estimator of
<span class="math inline">\(\boldsymbol{\theta}\)</span> if</p>
<p><span class="math display">\[L(\widehat{\boldsymbol{\theta}} \mid \boldsymbol{x}) \geq L(\boldsymbol{\theta} \mid \boldsymbol{x}), \forall \ \boldsymbol{\theta}\]</span>.</p>
<p>Since the logarithm function is monotonically increasing, for computational simplicity it
is usual use the log-likelihood function,
<span class="math display">\[\ell(\boldsymbol{\theta} \mid \boldsymbol{x}) = \log  L(\boldsymbol{\theta} \mid \boldsymbol{x}),\]</span>
since both function lead to the same maximum point.
Usually, the MLEs cannot be expressed in analytically formula, then they are obtained by
numerical maximization of the log-likelihood function.</p>
<p>The first derivative of log-likelihood function with respect to each parameter,
<span class="math inline">\(\boldsymbol{\theta}_j, j = 1, \ldots, p\)</span>, is known as score function. It can be shown
that
<span class="math display">\[\begin{equation*}
\mathbb{E}\left[\dfrac{\textrm{d}}{\textrm{d}\boldsymbol{\theta}_j}\,\ell(\boldsymbol{\theta} \mid \boldsymbol{x})\right] = 0,
\end{equation*}\]</span>
that is, the score function has mean zero.
Note that the score function is a vector of first partial derivatives, one for each
element of <span class="math inline">\(\boldsymbol{\theta}\)</span>. If the log-likelihood is concave, then the MLEs can be
obtained by setting the score function to vector zero and solving the system equation.</p>
<p>Another important quantity in the likelihood theory is the expected Fisher information
matrix, defined by
<span class="math display">\[\begin{equation}\label{eq:fisher-esp}
\mathbf{I}\left(\boldsymbol{\theta}\right) = \mathbb{E}\left [-\dfrac{\partial^2}{\partial\boldsymbol{\theta}_j\,\partial\boldsymbol{\theta}_k} \log f(x \mid \boldsymbol{\theta})\right]
\end{equation}\]</span>
for <span class="math inline">\(j, k = 1, \ldots, p\)</span>.</p>
<p>The drawback of working with <span class="math inline">\(\mathbf{I}\left(\boldsymbol{\theta}\right)\)</span> is the
difficult to obtain analytically expression for the expectation, since a
integral needs to be solved.</p>
<p>Under certain regularity conditions, the observed Fisher information matrix, given by
<span class="math display">\[\begin{equation}\label{eq:fisher-obs}
\mathbf{H}\left(\boldsymbol{\theta}\right) = -\dfrac{\partial}{\partial\boldsymbol{\theta}_j\,\partial\boldsymbol{\theta}_k} \log f(x_i \mid \boldsymbol{\theta}), %\at[\big]{\boldsymbol{\theta}=\widehat{\boldsymbol{\theta}}}
\end{equation}\]</span>
is an consistent estimator of <span class="math inline">\(\mathbf{I}\left(\boldsymbol{\theta}\right)\)</span>.</p>
<p>The concept of information is associate to the curvature of likelihood function and
the greater the curvature the more accurate the information contained in the likelihood,
the data!
In other words, the Fisher information matrix indicate the accuracy of estimates,
therefore it is essential for the construction of confidence intervals and
hypothesis tests.</p>
</div>
</div>
<div id="final-comments" class="section level2">
<h2>Final comments</h2>
<p>In this text, I tried to clarify the concept of statistical inference and the likelihood
paradigm.
In the sequel, I will explore the concept of bias and discus some methods for bias correction
of the maximum likelihood estimators in finite samples.</p>
</div>
<div id="references" class="section level2">
<h2>References</h2>
<p><a href="https://www.springer.com/gp/book/9783642552540">An Introduction to Bartlett Correction and Bias Reduction</a></p>
<p><a href="https://www.amazon.com.br/Statistical-Inference-George-Casella/dp/0534243126">Statistical Inference</a></p>
</div>
