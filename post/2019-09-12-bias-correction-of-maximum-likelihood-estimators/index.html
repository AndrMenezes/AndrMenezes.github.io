<script src="index_files/header-attrs-2.8/header-attrs.js"></script>


<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>The term bias is quite common in statistics to express a systematic error that can often
be identified and even corrected.
In the context of statistical inference the bias is related to an important property of
estimators.</p>
<p>As I discussed in this <a href="https://andrmenezes.github.io/post/2019-07-27-infer%C3%AAncia-estat%C3%ADstica-e-verossimilhan%C3%A7a/"><em>post</em></a>
the maximum likelihood method is widely used in practice, mainly because its intuitive
appeal and the properties it enjoys.</p>
<p>However, the statistical properties that make the maximum likelihood estimators (MLEs)
attractive are mostly asymptotic properties, which means holds for large sample size.
For instance, the MLEs have a bias of order <span class="math inline">\(\mathcal{O}(n^{-1})\)</span>, where <span class="math inline">\(n\)</span> is the
sample size, i.e., this bias decreases as sample size increases.
Thus, for small or moderate sample sizes the MLEs can be highly biased</p>
</div>
<div id="definition" class="section level2">
<h2>Definition</h2>
<p>Let be <span class="math inline">\(\widehat{\theta}\)</span> the MLE of the parameter <span class="math inline">\({\theta}\)</span>, then the bias of the estimator
is given by
<span class="math display">\[\mathcal{B}\left(\widehat{\theta}\right) = \mathbb{E}\left({\widehat{\theta}}\right) - \theta\]</span>
where <span class="math inline">\(\mathbb{E}(\cdot)\)</span> denotes the expectation with respect to the sampling distribution
of the estimator <span class="math inline">\(\widehat{\theta}\)</span>.</p>
<p>Usually, it is not possible to determine the MLE through a explicit formula, then we obtained
numerical estimate of <span class="math inline">\({\widehat{\theta}}\)</span> from the maximization of the log-likelihood
function.
In these cases, it is natural to think that it is not possible to find the bias of
<span class="math inline">\({\widehat{\theta}}\)</span>, but, there are at least three approaches in the literature for
maximum likelihood bias correction.</p>
<p>In the next section, I will briefly describe such approaches, interested readers may found
more technical details in my undergraduate research <a href="https://github.com/AndrMenezes/si2017/raw/master/final_report.pdf">here</a>.</p>
</div>
<div id="the-bias-correction-approaches" class="section level2">
<h2>The bias correction approaches</h2>
<p>By using second degree Taylor approximation on the score vector,
<a href="https://rss.onlinelibrary.wiley.com/doi/10.1111/j.2517-6161.1968.tb00724.x">Cox e Snell (1968)</a>
developed a methodology that allows to obtain analytical expression for the bias of MLEs
This method has been extensively explored in the literature for different statistical models.
I and professor <a href="https://orcid.org/0000-0001-6740-0445">Josmar Mazucheli</a>
developed several works deducing analytical expressions for the bias of the parameters of different statistical models.
The main highlight of this partnership is the paper
<a href="https://journal.r-project.org/archive/2017/RJ-2017-055/index.html">mle.tools: An R Package for Maximum Likelihood Bias Correction</a>,
published in <a href="https://journal.r-project.org/">R Journal</a>,
where we evaluated the efficiency of the <code>mle.tools</code> R package for Cox-Snell bias correction
on several probability distributions.</p>
<p>A second approach was proposed by
<a href="https://www.jstor.org/stable/2336755?seq=1#page_scan_tab_contents">Firth (1993)</a>
and it is known as preventive, because the author proposed to transform the score vector
before obtain the maximum likelihood estimates. For technical details on this methodology
I recommend the excellent book <a href="https://www.springer.com/gp/book/9783642552540">An Introduction to Bartlett Correction and Bias Reduction</a>
from professors Gauss and Cribari of UFPE.</p>
<p>Finnaly, it is worth mentioning that resampling methods such as Bootstrap and Jacknife
are computational alternatives that can be used for bias estimation.
A practical example can be found in my work <a href="https://github.com/AndrMenezes/sts2017/raw/master/work5/draft-5.pdf">here</a>.</p>
</div>
<div id="references" class="section level2">
<h2>References</h2>
<p><a href="https://rss.onlinelibrary.wiley.com/doi/10.1111/j.2517-6161.1968.tb00724.x">A General Definition of Residuals</a></p>
<p><a href="https://www.springer.com/gp/book/9783642552540">An Introduction to Bartlett Correction and Bias Reduction</a></p>
<p><a href="https://www.jstor.org/stable/2336755?seq=1#page_scan_tab_contents">Bias Reduction of Maximum Likelihood Estimates</a></p>
</div>
