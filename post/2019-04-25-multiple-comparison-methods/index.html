<script src="index_files/header-attrs-2.8/header-attrs.js"></script>


<p>Conducting planned experimental studies, the so-called experimental design, is very
common in areas such as, engineering, biology and medicine.
In general, the researcher has the interest in investigate how certain factors influence
some characteristic of interest, the response variable.</p>
<p>To compare different factors several methodologies can be used. The choice
depending on the nature of characteristic of interest was measure.
Some examples of techniques that can be used are:</p>
<ul>
<li><p>Comparisons of means: <a href="https://en.wikipedia.org/wiki/Student%27s_t-test">Student’s t-test</a></p></li>
<li><p>Comparisons of medians: <a href="https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test">Wilcoxon–Mann–Whitney test</a></p></li>
<li><p>Comparisons of contingency tables: <a href="https://en.wikipedia.org/wiki/Chi-squared_test">Chi-squared test</a>;</p></li>
<li><p>Comparisons of survival curves: <a href="https://en.wikipedia.org/wiki/Logrank_test">Logrank test</a>.</p></li>
</ul>
<p>Regardless the nature of characteristic of interest, in the comparison of several factors
simultaneously under the statistical point of view we are conducting <strong>simultaneously inference</strong>,
either through simultaneous confidence intervals or simultaneous hypothesis tests.</p>
<p>Consider the interest in comparing the means of <span class="math inline">\(k\)</span> factor, i.e., testing <span class="math inline">\(m = \dfrac{k\,(k- 1)}{2}\)</span>
hypotheses of the form
<span class="math display">\[
\mathcal{H}_0: \mu_i = \mu_j \quad \text{versus} \quad
\mathcal{H}_1: \mu_i \neq \mu_j
\]</span>
where <span class="math inline">\(i \neq j\)</span>.</p>
<p>When conduct a simple statistical hypothesis test it can happen the Type I error, i.e.,
the probability to reject the null hypothesis when it is true.
In case of multiple comparisons, there are two kinds of Type I error:</p>
<ul>
<li><p>Comparison-wise error rate <span class="math inline">\((\alpha)\)</span>: probability of making a Type I error in any comparison.</p></li>
<li><p>Family-wise error rate <span class="math inline">\((\alpha_f)\)</span>: probability of making one or more Type I error in a set (family) of comparisons.</p></li>
</ul>
<p>When the simultaneous tests are independents the relationship between these error rates is expressed by:
<span class="math display">\[
\alpha_f = 1 - (1 - \alpha)^m.
\]</span></p>
<p>The conventional hypothesis tests, e.g., the Student’s t-test and the Chi-squared test,
are not proposed to evaluate several hypoteses simultaneously, since such tests control only
the comparison-wise Type I error <span class="math inline">\((\alpha)\)</span>. Thus, when used in multiple comparisons problems
they inflate the family-wise Type I error <span class="math inline">\((\alpha_f)\)</span>, leading to misleading interpretations of the study.</p>
<p>Certainly the <a href="https://en.wikipedia.org/wiki/Bonferroni_correction">Bonferroni correction</a>
is the well known and perhaps the most used by
statistics users in order to overcome this issue.
However, several other corrections are available and have better statistical properties than
the Bonferroni correction.</p>
<p>In a joint work with <a href="https://twitter.com/vbfelix">Vinicius Félix</a>, we evaluate by
Monte Carlo simulations the power and family-wise Type I error rate of ten corrections applied
on Student’s t-test for means comparisons between groups in different scenarios.
We conclude that the Bonferroni correction was the second worst based on the evaluate
criteria and scenarios. The <a href="https://en.wikipedia.org/wiki/False_discovery_rate">Benjamini-Hochberg</a>
correction was provide the best performance among the ten corrections.</p>
<p>More details of the paper can be found <a href="/pdf/papers/2018__corrections_t-test.pdf">here</a>.
The codes used for the simulations are available <a href="https://github.com/AndrMenezes/mcp/tree/master/Scripts/paper">here</a></p>
<div id="references" class="section level2">
<h2>References</h2>
<p><a href="https://journal.r-project.org/archive/2016/RJ-2016-017/index.html">Calvo, B. and Santafé, G. (2016) scmamp: Statistical Comparison of Multiple Algorithms in Multiple Problems. The R Journal, 8(1), 248–256.</a></p>
<p><a href="http://siba-ese.unisalento.it/index.php/ejasa/article/view/17017">Félix, V. B. and Menezes, A. F. B. (2018) Comparisons of ten corrections methods for t-test in multiple comparisons via Monte Carlo study, 11(1), 74–91</a></p>
<p><a href="/pdf/Implementing%20Multiple%20Comparisons%20on%20Pearson%20Chi-square%20Test.pdf">Jin, M. and Wang, B. (2014) Implementing multiple comparisons on Pearson Chi-square test for RxC contingency table in SAS</a></p>
</div>
